{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "from urllib.request import urlopen\n",
    "from selenium import webdriver # allow launching browser\n",
    "from selenium.webdriver.common.by import By # allow search with parameters\n",
    "from selenium.webdriver.support.ui import WebDriverWait # allow waiting for page to load\n",
    "from selenium.webdriver.support import expected_conditions as EC # determine whether the web page has loaded\n",
    "from selenium.common.exceptions import TimeoutException # handling timeout situation\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "from datetime import date "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver_option = webdriver.ChromeOptions()\n",
    "driver_option.add_argument('--headless')\n",
    "# driver_option.add_argument('--incognito')\n",
    "chromedriver_path = '/Users/ellpeeaxe/Desktop/SMU/Y4S1/SA/chromedriver' # Change this to your own chromedriver path!\n",
    "\n",
    "def create_webdriver():\n",
    "    return webdriver.Chrome(executable_path=chromedriver_path, chrome_options=driver_option)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_posturls(username):\n",
    "    \n",
    "    browser = create_webdriver()\n",
    "    # Open the website\n",
    "    browser.get('https://www.instagram.com/'+username)\n",
    "\n",
    "    # Get the number of posts\n",
    "    post_num_element = browser.find_element_by_class_name('g47SY')\n",
    "    post_num_html = post_num_element.get_attribute('innerHTML')\n",
    "\n",
    "    # Retrieve number of posts\n",
    "    soup = BeautifulSoup(post_num_html, 'html.parser')\n",
    "    num_posts = int(soup.get_text().replace(',', ''))\n",
    "\n",
    "    urls = []\n",
    "    # Get scroll height\n",
    "    last_height = browser.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "    has_more = scroll(1,browser,last_height)\n",
    "\n",
    "    while len(urls) < num_posts:   \n",
    "\n",
    "        url_element = browser.find_elements_by_css_selector('div.v1Nh3.kIKUG._bz0w')\n",
    "\n",
    "        for url in url_element:\n",
    "            url = url.get_attribute('innerHTML')\n",
    "            soup = BeautifulSoup(url, 'html.parser')\n",
    "            urls.append(soup.find('a')['href'])\n",
    "\n",
    "        urls = list(dict.fromkeys(urls))\n",
    "        print(len(urls))\n",
    "        has_more = scroll(1,browser,last_height)\n",
    "        \n",
    "    print(username + \" \" + str(len(urls)))\n",
    "\n",
    "    browser.close()\n",
    "\n",
    "    return urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scroll(times,browser,last_height):\n",
    "    SCROLL_PAUSE_TIME = 3\n",
    "\n",
    "    scrolling = times\n",
    "    while scrolling > 0:\n",
    "        # Scroll down to bottom\n",
    "        browser.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "\n",
    "        # Wait to load page\n",
    "        time.sleep(SCROLL_PAUSE_TIME)\n",
    "\n",
    "        # Calculate new scroll height and compare with last scroll height\n",
    "        new_height = browser.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_height == last_height:\n",
    "            return False\n",
    "        last_height = new_height\n",
    "        scrolling -= 1\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_likescomments(urls):\n",
    "    posts = []\n",
    "    dates = []\n",
    "    likes = []\n",
    "    comment_count = []\n",
    "    commenters = []\n",
    "    comments = []\n",
    "    \n",
    "    for url in list(urls):\n",
    "        url = 'http://instagram.com' + url\n",
    "        print(url)\n",
    "        page = urlopen(url).read()\n",
    "        soup = BeautifulSoup(page, features=\"html.parser\")\n",
    "\n",
    "        string = soup.find(\"meta\",  property=\"og:description\")['content']\n",
    "        string = string.split(\" \")\n",
    "        num_likes = string[0]\n",
    "        num_comments = string[2]\n",
    "\n",
    "        browser = create_webdriver()\n",
    "        browser.get(url)\n",
    "        \n",
    "        date = browser.find_element_by_css_selector('time._1o9PC.Nzb55').get_attribute('datetime')\n",
    "        print(date)\n",
    "\n",
    "        hasLoadMore = True\n",
    "        while hasLoadMore:\n",
    "            time.sleep(2)\n",
    "            try:\n",
    "                browser.find_element_by_css_selector('button.dCJp8.afkep > span.glyphsSpriteCircle_add__outline__24__grey_9.u-__7').click()\n",
    "            except:\n",
    "                hasLoadMore = False\n",
    "\n",
    "        texts = browser.find_elements_by_class_name('C4VMK')\n",
    "       \n",
    "        for comment in texts:\n",
    "            posts.append(url)\n",
    "            dates.append(date)\n",
    "            likes.append(num_likes)\n",
    "            comment_count.append(num_comments)\n",
    "            try:\n",
    "                comments.append(comment.text.split(\"\\n\")[1])\n",
    "                commenters.append(comment.text.split(\"\\n\")[0])\n",
    "            except:\n",
    "                commenters.append(\" \")\n",
    "                comments.append(\" \")\n",
    "\n",
    "        browser.close()\n",
    "\n",
    "    df = pd.DataFrame({'url': posts, \n",
    "                       'num_likes': likes, \n",
    "                       'date' : dates,\n",
    "                       'num_comment': comment_count, \n",
    "                       'commenter': commenters, \n",
    "                       'comment': comments})\n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(os.getcwd()+\"/data/LikesComments/\"):\n",
    "    os.makedirs(os.getcwd()+\"/data/LikesComments/\")\n",
    "    \n",
    "for username in open(\"usernames.txt\", \"r\"):\n",
    "    urls = get_posturls(username.strip())\n",
    "    df = scrape_likescomments(urls)\n",
    "    df['username'] = username.strip()\n",
    "    \n",
    "    display(df)\n",
    "    \n",
    "    df.to_csv(os.getcwd()+\"/data/LikesComments/\"+username.strip()+\"-\"+str(date.today())+'.txt',sep='\\t',index = False,encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "username = \"seedlysg\"\n",
    "urls = get_posturls(username.strip())\n",
    "df = scrape_likescomments(urls)\n",
    "df['username'] = username.strip()\n",
    "\n",
    "df.to_csv(os.getcwd()+\"/data/LikesComments/\"+username.strip()+\"-\"+str(date.today())+'.txt',sep='\\t',index = False,encoding='utf-8')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
